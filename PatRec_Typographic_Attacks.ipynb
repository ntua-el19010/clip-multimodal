{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tevJoPksqGoE"
      },
      "source": [
        "# Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install necessary packages and import CLIP model"
      ],
      "metadata": {
        "id": "B45cqwaIZ3xT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KO_gnX27n5fO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSBHsAaNp8I_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import clip\n",
        "import torch\n",
        "import random\n",
        "import requests\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from pkg_resources import packaging\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shaWybJWqA_x"
      },
      "outputs": [],
      "source": [
        "model, preprocess = clip.load(\"RN50x4\")\n",
        "model.cuda().eval()\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alePijoXy6AH"
      },
      "source": [
        "# Image preprocessing\n",
        "\n",
        "\n",
        "Prepare different image versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_8vjaR8hCbg"
      },
      "outputs": [],
      "source": [
        "def add_text(img, text, coordinates, font_scale, font_thickness, outline_thickness):\n",
        "    font = cv2.FONT_HERSHEY_DUPLEX\n",
        "    # Draw black outline\n",
        "    cv2.putText(img, text, coordinates, font, font_scale, (0, 0, 0), outline_thickness)\n",
        "    # Draw text on top of the outline\n",
        "    cv2.putText(img, text, coordinates, font, font_scale, (255, 255, 255), font_thickness)\n",
        "\n",
        "def calculate_changed_pixels(original_img, modified_img):\n",
        "    diff_img = cv2.absdiff(original_img, modified_img)\n",
        "    changed_pixels = np.count_nonzero(diff_img)\n",
        "    total_pixels = original_img.shape[0] * original_img.shape[1]\n",
        "    percentage_changed = (changed_pixels / total_pixels) * 100\n",
        "    return percentage_changed\n",
        "\n",
        "def calculate_text_size(img, text, font_scale, font_thickness):\n",
        "    font = cv2.FONT_HERSHEY_DUPLEX\n",
        "    text_size, _ = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
        "    return text_size\n",
        "\n",
        "def calculate_coordinates(img, text, font_scale, font_thickness, num_rows, num_cols, buffer=50):\n",
        "    text_size = calculate_text_size(img, text, font_scale, font_thickness)\n",
        "    cell_width = (img.shape[1] - 2 * buffer) // num_cols\n",
        "    cell_height = (img.shape[0] - 2 * buffer) // num_rows\n",
        "    coordinates = []\n",
        "\n",
        "    for row in range(num_rows):\n",
        "        for col in range(num_cols):\n",
        "            cell_x = buffer + col * cell_width\n",
        "            cell_y = buffer + row * cell_height\n",
        "            cell_center_x = cell_x + cell_width // 2\n",
        "            cell_center_y = cell_y + cell_height // 2\n",
        "            rand_x = random.randint(cell_x, cell_x + cell_width - text_size[0])\n",
        "            rand_y = random.randint(cell_y + text_size[1], cell_y + cell_height - text_size[1])\n",
        "            coordinates.append((rand_x, rand_y))\n",
        "\n",
        "    return coordinates\n",
        "\n",
        "def process_images(input_directory, text_to_add, coordinates = None):\n",
        "    images = []\n",
        "    images_text = []\n",
        "\n",
        "    output_directory = f\"{input_directory}_{text_to_add}\"\n",
        "    # Create the output directory if it doesn't exist\n",
        "    if not os.path.exists(output_directory):\n",
        "        os.makedirs(output_directory)\n",
        "\n",
        "    # Define the number of rows and columns for the grid\n",
        "    num_rows, num_cols = 4, 2\n",
        "\n",
        "    # Calculate the size of each text\n",
        "    font_scale = 0.6\n",
        "    font_thickness = 1\n",
        "\n",
        "    percentage, count = 0, 0\n",
        "\n",
        "    # Initialize coordinates for the whole directory\n",
        "    if not coordinates:\n",
        "        coordinates = calculate_coordinates(cv2.imread(os.path.join(input_directory, os.listdir(input_directory)[0])), text_to_add, font_scale, font_thickness, num_rows, num_cols)\n",
        "\n",
        "    # Process each image in the input directory\n",
        "    for filename in tqdm(os.listdir(input_directory)):\n",
        "        if filename.endswith(\".JPEG\"):  # Process only image files\n",
        "            count += 1\n",
        "            # Load the input image\n",
        "            input_image_path = os.path.join(input_directory, filename)\n",
        "            input_image = cv2.imread(input_image_path)\n",
        "\n",
        "            # Add text to the image at precalculated coordinates\n",
        "            modified_image = input_image.copy()\n",
        "            outline_thickness = 3\n",
        "            for coord in coordinates:\n",
        "                add_text(modified_image, text_to_add, coord, font_scale, font_thickness, outline_thickness)\n",
        "\n",
        "            # Save the modified image to the output directory\n",
        "            output_image_path = os.path.join(output_directory, filename)\n",
        "            cv2.imwrite(output_image_path, modified_image)\n",
        "\n",
        "            # Calculate the percentage of changed pixels\n",
        "            percentage_changed = calculate_changed_pixels(input_image, modified_image)\n",
        "            percentage += percentage_changed\n",
        "\n",
        "            # Preprocess images to use as input for CLIP model\n",
        "            image = Image.open(input_image_path).convert('RGB')\n",
        "            images.append(preprocess(image))\n",
        "\n",
        "            image = Image.open(output_image_path).convert('RGB')\n",
        "            images_text.append(preprocess(image))\n",
        "\n",
        "    average_percent_changed = percentage/count\n",
        "    print(f\"Processed {count} images. Average percentage of changed pixels: {average_percent_changed:.2f}%\")\n",
        "\n",
        "    return coordinates, images, images_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZ7Xx9_qholw"
      },
      "outputs": [],
      "source": [
        "input_directory = \"/content/drive/MyDrive/patrec/ImageNetValidation\"\n",
        "text_to_add = \"radio\"\n",
        "\n",
        "# We used the same coordinates for different words\n",
        "coords = [(41, 72), (167, 149), (278, 96), (447, 76), (15, 349), (153, 310), (293, 327), (409, 315)]\n",
        "coords, images, images_text = process_images(input_directory, text_to_add, coordinates=coords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpHUeRI0sahU"
      },
      "source": [
        "# Typographic Attacks\n",
        "\n",
        "\n",
        "Using Zero-Shot Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7cKkPIdCdXh"
      },
      "outputs": [],
      "source": [
        "imagenet_labels = requests.get(\"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\").json()\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "# Define a function to process a batch of images\n",
        "def process_batch(images):\n",
        "    image_input = torch.tensor(np.stack(images)).cuda()\n",
        "\n",
        "    # Extract image features\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image_input).float()\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    text_descriptions = [f\"This is a photo of a {label}\" for label in imagenet_labels]\n",
        "    text_tokens = clip.tokenize(text_descriptions).cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(text_tokens).float()\n",
        "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "    top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)\n",
        "\n",
        "    return top_probs, top_labels\n",
        "\n",
        "# Iterate over batches of images\n",
        "all_top_probs = []\n",
        "all_top_labels = []\n",
        "all_top_probs_text = []\n",
        "all_top_labels_text = []\n",
        "for i in range(0, len(images), batch_size):\n",
        "    batch_images = images[i:i+batch_size]\n",
        "    top_probs, top_labels = process_batch(batch_images)\n",
        "    all_top_probs.append(top_probs)\n",
        "    all_top_labels.append(top_labels)\n",
        "\n",
        "    batch_images = images_text[i:i+batch_size]\n",
        "    top_probs, top_labels = process_batch(batch_images)\n",
        "    all_top_probs_text.append(top_probs)\n",
        "    all_top_labels_text.append(top_labels)\n",
        "\n",
        "# Concatenate results from all batches\n",
        "all_top_probs = torch.cat(all_top_probs, dim=0)\n",
        "all_top_labels = torch.cat(all_top_labels, dim=0)\n",
        "all_top_probs_text = torch.cat(all_top_probs_text, dim=0)\n",
        "all_top_labels_text = torch.cat(all_top_labels_text, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptV6vxPfG7Ox"
      },
      "outputs": [],
      "source": [
        "category = \"radio\"\n",
        "success = 0\n",
        "\n",
        "# Calculate success rate of attack\n",
        "for i in range(0, len(images)):\n",
        "    if category != imagenet_labels[all_top_labels[i].numpy()[0]] and category == imagenet_labels[all_top_labels_text[i].numpy()[0]]:\n",
        "        success += 1\n",
        "print(f\"Processed {len(images)} images. Success rate of typographic attack with text '{text_to_add}': {success/len(images)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np9iEsBqYBBg"
      },
      "outputs": [],
      "source": [
        "# Used to find ossible attack texts htat are short\n",
        "imagenet_labels = requests.get(\"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\").json()\n",
        "for label in imagenet_labels:\n",
        "    if len(label) <= 5:\n",
        "      print(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvLVWR134Cno"
      },
      "source": [
        "# Stroop Effect\n",
        "\n",
        "\n",
        "Using zero-shot classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kwnIDaiZwIy"
      },
      "outputs": [],
      "source": [
        "output_directory = \"/content/drive/MyDrive/patrec/StroopEffect\"\n",
        "# Create the output directory if it doesn't exist\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "def create_pattern_image(text, text_color):\n",
        "    # Define image dimensions\n",
        "    width, height = 400, 200\n",
        "\n",
        "    # Create a white background image\n",
        "    image = np.ones((height, width, 3), dtype=np.uint8) * 255\n",
        "\n",
        "    # Choose font and scale\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2\n",
        "\n",
        "    # Get the size of the text\n",
        "    text_size = cv2.getTextSize(text, font, font_scale, 2)[0]\n",
        "\n",
        "    # Calculate text position to be centered\n",
        "    text_x = (width - text_size[0]) // 2\n",
        "    text_y = (height + text_size[1]) // 2\n",
        "\n",
        "    # Write the text on the image with specified color\n",
        "    cv2.putText(image, text, (text_x, text_y), font, font_scale, text_color, 2, cv2.LINE_AA)\n",
        "\n",
        "    return image\n",
        "\n",
        "def save_image(image, text, color_name):\n",
        "    # Save the image to a file\n",
        "    dir = os.path.join(output_directory, text)\n",
        "    if not os.path.exists(dir):\n",
        "        os.makedirs(dir)\n",
        "    filename = f\"{color_name}.png\"\n",
        "    filepath = os.path.join(dir, filename)\n",
        "    cv2.imwrite(filepath, image)\n",
        "    print(f\"Image with {color_name} text saved as {filename}\")\n",
        "\n",
        "# List of colors and their corresponding BGR values\n",
        "colors = {\n",
        "    \"red\": (0, 0, 255),\n",
        "    \"blue\": (255, 0, 0),\n",
        "    \"green\": (0, 255, 0),\n",
        "    \"yellow\": (0, 255, 255),\n",
        "    \"orange\": (0, 165, 255),\n",
        "    \"purple\": (128, 0, 128),\n",
        "    \"black\": (0, 0, 0),\n",
        "    \"pink\": (203, 192, 255)\n",
        "}\n",
        "\n",
        "# Generate and save images for each color\n",
        "text = \"purple\"\n",
        "for color_name, color_value in colors.items():\n",
        "    pattern_image = create_pattern_image(text, color_value)\n",
        "    save_image(pattern_image, text, color_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckjlhKyIc4j3"
      },
      "outputs": [],
      "source": [
        "labels = ['black', 'red', 'green', 'blue', 'orange', 'yellow', 'purple', 'pink']\n",
        "output_directory = \"/content/drive/MyDrive/patrec/StroopEffect\"\n",
        "batch_size = 8\n",
        "\n",
        "# Load and preprocess the images\n",
        "original_images = []\n",
        "images = []\n",
        "\n",
        "directory = os.path.join(output_directory, \"pink\")\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".png\"):  # Process only image files\n",
        "      image = Image.open(os.path.join(directory, filename)).convert(\"RGB\")\n",
        "\n",
        "      original_images.append(image)\n",
        "      images.append(preprocess(image))\n",
        "\n",
        "image_input = torch.tensor(np.stack(images)).cuda()\n",
        "\n",
        "# Extract image features\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "text_descriptions = [f\"My favorite word, written in the color {label}\" for label in labels]\n",
        "text_tokens = clip.tokenize(text_descriptions).cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text_tokens).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "top_probs, top_labels = text_probs.cpu().topk(3, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4, 14))\n",
        "\n",
        "for i, image in enumerate(original_images):\n",
        "    plt.subplot(8, 2, 2 * i + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(8, 2, 2 * i + 2)\n",
        "    y = np.arange(top_probs.shape[-1])\n",
        "    plt.grid()\n",
        "    bars = plt.barh(y, top_probs[i], color='gray')\n",
        "\n",
        "    max_index = np.argmax(top_probs[i])\n",
        "    bars[max_index].set_color('darkgreen')\n",
        "\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.gca().set_axisbelow(True)\n",
        "    plt.yticks(y, [labels[index] for index in top_labels[i]])\n",
        "    plt.xlabel(\"probability\")\n",
        "\n",
        "plt.subplots_adjust(wspace=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bMmskEx8P94b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}