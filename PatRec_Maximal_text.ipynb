{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization"
      ],
      "metadata": {
        "id": "79yysQnU-GgE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6In8XSCSIdK"
      },
      "outputs": [],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from pkg_resources import packaging\n",
        "import matplotlib.pyplot as plt\n",
        "import clip\n",
        "from PIL import Image\n",
        "import os\n",
        "from glob import glob\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the CLIP model and freeze all its parameters\n",
        "\n",
        "model, preprocess = clip.load(\"RN50x4\")\n",
        "model.eval()\n",
        "model.trainable = False\n",
        "m = model.requires_grad_(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrZVKdi9_wTe",
        "outputId": "9c4b1c8a-0c4b-4456-fcf8-dc6f86ae3a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 402M/402M [00:04<00:00, 86.4MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# Download some images from drive and classify them in folders\n",
        "\n",
        "!wget https://docs.google.com/uc?id=1jP081zYPv1AKVwrOg2UZFlU_nPd8lLS5 -O /content/trump1.zip\n",
        "!wget https://docs.google.com/uc?id=1yUDmA07-519KOpTEHbEebZR8SLrcVhKC -O /content/trump2.zip\n",
        "!wget https://docs.google.com/uc?id=196fkPNzgoayxlTg06yEf43YO13QLO2YG -O /content/usa.zip\n",
        "!wget https://docs.google.com/uc?id=1pbkUuPjVtAr6EKJaadBAqk6l6Y-xodZd -O /content/flags.zip\n",
        "!wget https://docs.google.com/uc?id=1eZS_7seR9j7iSWrKINPfKI9fhKLElVpw -O /content/dogs.zip\n",
        "\n",
        "!rm -r /content/trump1\n",
        "!mkdir /content/trump1                              # unzip trump1.zip to new trump1 directory\n",
        "!unzip /content/trump1.zip -d /content/trump1\n",
        "trump_files1 = glob(os.path.join('/content/trump1', \"**\"))   # get the paths of all files in trump1 directory\n",
        "\n",
        "!rm -r /content/trump2\n",
        "!mkdir /content/trump2\n",
        "!unzip /content/trump2.zip -d /content/trump2\n",
        "trump_files2 = glob(os.path.join('/content/trump2', \"**\"))\n",
        "\n",
        "!rm -r /content/usa\n",
        "!mkdir /content/usa\n",
        "!unzip /content/usa.zip -d /content/usa\n",
        "usa_files = glob(os.path.join('/content/usa', \"**\"))\n",
        "\n",
        "!rm -r /content/flags\n",
        "!mkdir /content/flags\n",
        "!unzip /content/flags.zip -d /content/flags\n",
        "flags_files = glob(os.path.join('/content/flags', \"**\"))\n",
        "\n",
        "!rm -r /content/dogs\n",
        "!mkdir /content/dogs\n",
        "!unzip /content/dogs.zip -d /content/dogs\n",
        "dogs_files = glob(os.path.join('/content/dogs', \"**\"))\n",
        "\n",
        "#!rm -r /content/myanimals\n",
        "#!mkdir /content/myanimals\n",
        "#!unzip /content/drive/MyDrive/myanimals.zip -d /content/myanimals\n",
        "#myanimals_files = glob(os.path.join('/content/myanimals/myanimals', \"**\"))"
      ],
      "metadata": {
        "id": "eK-K6eq9IV-Y",
        "outputId": "11b1440a-7185-4a14-980a-906b6131f5e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-04 14:31:04--  https://docs.google.com/uc?id=1jP081zYPv1AKVwrOg2UZFlU_nPd8lLS5\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.2.101, 142.251.2.100, 142.251.2.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.2.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1jP081zYPv1AKVwrOg2UZFlU_nPd8lLS5 [following]\n",
            "--2024-03-04 14:31:04--  https://drive.usercontent.google.com/download?id=1jP081zYPv1AKVwrOg2UZFlU_nPd8lLS5\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.101.132, 2607:f8b0:4023:c06::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.101.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 475559 (464K) [application/octet-stream]\n",
            "Saving to: â€˜/content/trump1.zipâ€™\n",
            "\n",
            "/content/trump1.zip 100%[===================>] 464.41K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-03-04 14:31:06 (3.79 MB/s) - â€˜/content/trump1.zipâ€™ saved [475559/475559]\n",
            "\n",
            "--2024-03-04 14:31:06--  https://docs.google.com/uc?id=1yUDmA07-519KOpTEHbEebZR8SLrcVhKC\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.2.101, 142.251.2.100, 142.251.2.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.2.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1yUDmA07-519KOpTEHbEebZR8SLrcVhKC [following]\n",
            "--2024-03-04 14:31:06--  https://drive.usercontent.google.com/download?id=1yUDmA07-519KOpTEHbEebZR8SLrcVhKC\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.101.132, 2607:f8b0:4023:c06::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.101.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 887556 (867K) [application/octet-stream]\n",
            "Saving to: â€˜/content/trump2.zipâ€™\n",
            "\n",
            "/content/trump2.zip 100%[===================>] 866.75K  4.70MB/s    in 0.2s    \n",
            "\n",
            "2024-03-04 14:31:07 (4.70 MB/s) - â€˜/content/trump2.zipâ€™ saved [887556/887556]\n",
            "\n",
            "--2024-03-04 14:31:07--  https://docs.google.com/uc?id=196fkPNzgoayxlTg06yEf43YO13QLO2YG\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.2.101, 142.251.2.100, 142.251.2.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.2.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=196fkPNzgoayxlTg06yEf43YO13QLO2YG [following]\n",
            "--2024-03-04 14:31:07--  https://drive.usercontent.google.com/download?id=196fkPNzgoayxlTg06yEf43YO13QLO2YG\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.101.132, 2607:f8b0:4023:c06::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.101.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 174735 (171K) [application/octet-stream]\n",
            "Saving to: â€˜/content/usa.zipâ€™\n",
            "\n",
            "/content/usa.zip    100%[===================>] 170.64K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-03-04 14:31:08 (1.93 MB/s) - â€˜/content/usa.zipâ€™ saved [174735/174735]\n",
            "\n",
            "--2024-03-04 14:31:08--  https://docs.google.com/uc?id=1pbkUuPjVtAr6EKJaadBAqk6l6Y-xodZd\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.2.101, 142.251.2.100, 142.251.2.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.2.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1pbkUuPjVtAr6EKJaadBAqk6l6Y-xodZd [following]\n",
            "--2024-03-04 14:31:08--  https://drive.usercontent.google.com/download?id=1pbkUuPjVtAr6EKJaadBAqk6l6Y-xodZd\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.101.132, 2607:f8b0:4023:c06::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.101.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 488237 (477K) [application/octet-stream]\n",
            "Saving to: â€˜/content/flags.zipâ€™\n",
            "\n",
            "/content/flags.zip  100%[===================>] 476.79K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-03-04 14:31:11 (4.40 MB/s) - â€˜/content/flags.zipâ€™ saved [488237/488237]\n",
            "\n",
            "--2024-03-04 14:31:11--  https://docs.google.com/uc?id=1eZS_7seR9j7iSWrKINPfKI9fhKLElVpw\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.2.101, 142.251.2.100, 142.251.2.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.2.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1eZS_7seR9j7iSWrKINPfKI9fhKLElVpw [following]\n",
            "--2024-03-04 14:31:11--  https://drive.usercontent.google.com/download?id=1eZS_7seR9j7iSWrKINPfKI9fhKLElVpw\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.101.132, 2607:f8b0:4023:c06::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.101.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8385553 (8.0M) [application/octet-stream]\n",
            "Saving to: â€˜/content/dogs.zipâ€™\n",
            "\n",
            "/content/dogs.zip   100%[===================>]   8.00M  25.9MB/s    in 0.3s    \n",
            "\n",
            "2024-03-04 14:31:15 (25.9 MB/s) - â€˜/content/dogs.zipâ€™ saved [8385553/8385553]\n",
            "\n",
            "rm: cannot remove '/content/trump1': No such file or directory\n",
            "Archive:  /content/trump1.zip\n",
            " extracting: /content/trump1/Election_2024_Trump_90647-3489f.webp  \n",
            "  inflating: /content/trump1/former-u-s-president-donald-trump-speaks-to-reporters-news-photo-1680289458.jpg  \n",
            "  inflating: /content/trump1/im-799917.jfif  \n",
            "  inflating: /content/trump1/images.jfif  \n",
            "  inflating: /content/trump1/skynews-donald-trump_6398780.jpg  \n",
            "  inflating: /content/trump1/TrumpIowa-2048x1365-1-1024x683.jpg  \n",
            "rm: cannot remove '/content/trump2': No such file or directory\n",
            "Archive:  /content/trump2.zip\n",
            "  inflating: /content/trump2/00TRUMPLETTERS-web-superJumbo.jpg  \n",
            "  inflating: /content/trump2/71l1vmdSYDL._AC_UF1000,1000_QL80_.jpg  \n",
            "  inflating: /content/trump2/AP.Trump_-1200x800.webp  \n",
            "  inflating: /content/trump2/download.jfif  \n",
            "  inflating: /content/trump2/images (1).jfif  \n",
            "  inflating: /content/trump2/images.jfif  \n",
            "  inflating: /content/trump2/s-l1200.webp  \n",
            "  inflating: /content/trump2/trump-flag-make-america-great-again-red-3x5-outdoor-flags-for-sale.jpg  \n",
            "rm: cannot remove '/content/usa': No such file or directory\n",
            "Archive:  /content/usa.zip\n",
            "  inflating: /content/usa/About_the_USA_NYC_Statue_Liberty_._CROP_Web72DPI.jpg  \n",
            " extracting: /content/usa/download (1).jfif  \n",
            "  inflating: /content/usa/download (2).jfif  \n",
            "  inflating: /content/usa/download.jfif  \n",
            " extracting: /content/usa/download.png  \n",
            "  inflating: /content/usa/images.jfif  \n",
            "  inflating: /content/usa/Map_of_USA_with_state_names_2.svg.webp  \n",
            "rm: cannot remove '/content/flags': No such file or directory\n",
            "Archive:  /content/flags.zip\n",
            "  inflating: /content/flags/ad.png   \n",
            "  inflating: /content/flags/ae.png   \n",
            "  inflating: /content/flags/af.png   \n",
            "  inflating: /content/flags/ag.png   \n",
            " extracting: /content/flags/ai.png   \n",
            "  inflating: /content/flags/al.png   \n",
            "  inflating: /content/flags/am.png   \n",
            " extracting: /content/flags/ao.png   \n",
            " extracting: /content/flags/aq.png   \n",
            "  inflating: /content/flags/ar.png   \n",
            " extracting: /content/flags/as.png   \n",
            "  inflating: /content/flags/at.png   \n",
            " extracting: /content/flags/au.png   \n",
            " extracting: /content/flags/aw.png   \n",
            "  inflating: /content/flags/ax.png   \n",
            "  inflating: /content/flags/az.png   \n",
            "  inflating: /content/flags/ba.png   \n",
            "  inflating: /content/flags/bb.png   \n",
            " extracting: /content/flags/bd.png   \n",
            "  inflating: /content/flags/be.png   \n",
            "  inflating: /content/flags/bf.png   \n",
            "  inflating: /content/flags/bg.png   \n",
            " extracting: /content/flags/bh.png   \n",
            "  inflating: /content/flags/bi.png   \n",
            "  inflating: /content/flags/bj.png   \n",
            " extracting: /content/flags/bl.png   \n",
            " extracting: /content/flags/bm.png   \n",
            " extracting: /content/flags/bn.png   \n",
            "  inflating: /content/flags/bo.png   \n",
            "  inflating: /content/flags/bq.png   \n",
            "  inflating: /content/flags/br.png   \n",
            " extracting: /content/flags/bs.png   \n",
            "  inflating: /content/flags/bt.png   \n",
            "  inflating: /content/flags/bv.png   \n",
            "  inflating: /content/flags/bw.png   \n",
            " extracting: /content/flags/by.png   \n",
            "  inflating: /content/flags/bz.png   \n",
            " extracting: /content/flags/ca.png   \n",
            " extracting: /content/flags/cc.png   \n",
            "  inflating: /content/flags/cd.png   \n",
            "  inflating: /content/flags/cf.png   \n",
            "  inflating: /content/flags/cg.png   \n",
            "  inflating: /content/flags/ch.png   \n",
            "  inflating: /content/flags/ci.png   \n",
            " extracting: /content/flags/ck.png   \n",
            "  inflating: /content/flags/cl.png   \n",
            "  inflating: /content/flags/cm.png   \n",
            " extracting: /content/flags/cn.png   \n",
            "  inflating: /content/flags/co.png   \n",
            "  inflating: /content/flags/cr.png   \n",
            " extracting: /content/flags/cu.png   \n",
            " extracting: /content/flags/cv.png   \n",
            " extracting: /content/flags/cw.png   \n",
            " extracting: /content/flags/cx.png   \n",
            "  inflating: /content/flags/cy.png   \n",
            "  inflating: /content/flags/cz.png   \n",
            "  inflating: /content/flags/de.png   \n",
            "  inflating: /content/flags/dj.png   \n",
            "  inflating: /content/flags/dk.png   \n",
            "  inflating: /content/flags/dm.png   \n",
            "  inflating: /content/flags/do.png   \n",
            "  inflating: /content/flags/dz.png   \n",
            "  inflating: /content/flags/ec.png   \n",
            "  inflating: /content/flags/ee.png   \n",
            "  inflating: /content/flags/eg.png   \n",
            "  inflating: /content/flags/eh.png   \n",
            "  inflating: /content/flags/er.png   \n",
            "  inflating: /content/flags/es.png   \n",
            "  inflating: /content/flags/et.png   \n",
            "  inflating: /content/flags/fi.png   \n",
            "  inflating: /content/flags/fj.png   \n",
            " extracting: /content/flags/fk.png   \n",
            " extracting: /content/flags/fm.png   \n",
            "  inflating: /content/flags/fo.png   \n",
            "  inflating: /content/flags/fr.png   \n",
            "  inflating: /content/flags/ga.png   \n",
            "  inflating: /content/flags/gb-eng.png  \n",
            "  inflating: /content/flags/gb-nir.png  \n",
            " extracting: /content/flags/gb-sct.png  \n",
            "  inflating: /content/flags/gb-wls.png  \n",
            " extracting: /content/flags/gb.png   \n",
            " extracting: /content/flags/gd.png   \n",
            "  inflating: /content/flags/ge.png   \n",
            "  inflating: /content/flags/gf.png   \n",
            "  inflating: /content/flags/gg.png   \n",
            "  inflating: /content/flags/gh.png   \n",
            "  inflating: /content/flags/gi.png   \n",
            " extracting: /content/flags/gl.png   \n",
            "  inflating: /content/flags/gm.png   \n",
            "  inflating: /content/flags/gn.png   \n",
            " extracting: /content/flags/gp.png   \n",
            "  inflating: /content/flags/gq.png   \n",
            "  inflating: /content/flags/gr.png   \n",
            " extracting: /content/flags/gs.png   \n",
            "  inflating: /content/flags/gt.png   \n",
            "  inflating: /content/flags/gu.png   \n",
            "  inflating: /content/flags/gw.png   \n",
            " extracting: /content/flags/gy.png   \n",
            "  inflating: /content/flags/hk.png   \n",
            " extracting: /content/flags/hm.png   \n",
            " extracting: /content/flags/hn.png   \n",
            "  inflating: /content/flags/hr.png   \n",
            "  inflating: /content/flags/ht.png   \n",
            "  inflating: /content/flags/hu.png   \n",
            "  inflating: /content/flags/id.png   \n",
            "  inflating: /content/flags/ie.png   \n",
            "  inflating: /content/flags/il.png   \n",
            " extracting: /content/flags/im.png   \n",
            "  inflating: /content/flags/in.png   \n",
            " extracting: /content/flags/io.png   \n",
            "  inflating: /content/flags/iq.png   \n",
            "  inflating: /content/flags/ir.png   \n",
            "  inflating: /content/flags/is.png   \n",
            "  inflating: /content/flags/it.png   \n",
            " extracting: /content/flags/je.png   \n",
            " extracting: /content/flags/jm.png   \n",
            "  inflating: /content/flags/jo.png   \n",
            "  inflating: /content/flags/jp.png   \n",
            "  inflating: /content/flags/ke.png   \n",
            "  inflating: /content/flags/kg.png   \n",
            "  inflating: /content/flags/kh.png   \n",
            " extracting: /content/flags/ki.png   \n",
            "  inflating: /content/flags/km.png   \n",
            "  inflating: /content/flags/kn.png   \n",
            " extracting: /content/flags/kp.png   \n",
            " extracting: /content/flags/kr.png   \n",
            "  inflating: /content/flags/kw.png   \n",
            " extracting: /content/flags/ky.png   \n",
            " extracting: /content/flags/kz.png   \n",
            "  inflating: /content/flags/la.png   \n",
            "  inflating: /content/flags/lb.png   \n",
            " extracting: /content/flags/lc.png   \n",
            "  inflating: /content/flags/li.png   \n",
            " extracting: /content/flags/lk.png   \n",
            "  inflating: /content/flags/lr.png   \n",
            "  inflating: /content/flags/ls.png   \n",
            "  inflating: /content/flags/lt.png   \n",
            "  inflating: /content/flags/lu.png   \n",
            "  inflating: /content/flags/lv.png   \n",
            "  inflating: /content/flags/ly.png   \n",
            " extracting: /content/flags/ma.png   \n",
            "  inflating: /content/flags/mc.png   \n",
            "  inflating: /content/flags/md.png   \n",
            "  inflating: /content/flags/me.png   \n",
            "  inflating: /content/flags/mf.png   \n",
            "  inflating: /content/flags/mg.png   \n",
            " extracting: /content/flags/mh.png   \n",
            " extracting: /content/flags/mk.png   \n",
            "  inflating: /content/flags/ml.png   \n",
            "  inflating: /content/flags/mm.png   \n",
            " extracting: /content/flags/mn.png   \n",
            "  inflating: /content/flags/mo.png   \n",
            " extracting: /content/flags/mp.png   \n",
            "  inflating: /content/flags/mq.png   \n",
            "  inflating: /content/flags/mr.png   \n",
            "  inflating: /content/flags/ms.png   \n",
            "  inflating: /content/flags/mt.png   \n",
            "  inflating: /content/flags/mu.png   \n",
            "  inflating: /content/flags/mv.png   \n",
            "  inflating: /content/flags/mw.png   \n",
            "  inflating: /content/flags/mx.png   \n",
            " extracting: /content/flags/my.png   \n",
            "  inflating: /content/flags/mz.png   \n",
            "  inflating: /content/flags/na.png   \n",
            "  inflating: /content/flags/nc.png   \n",
            "  inflating: /content/flags/ne.png   \n",
            " extracting: /content/flags/nf.png   \n",
            "  inflating: /content/flags/ng.png   \n",
            "  inflating: /content/flags/ni.png   \n",
            "  inflating: /content/flags/nl.png   \n",
            "  inflating: /content/flags/no.png   \n",
            "  inflating: /content/flags/np.png   \n",
            "  inflating: /content/flags/nr.png   \n",
            "  inflating: /content/flags/nu.png   \n",
            " extracting: /content/flags/nz.png   \n",
            "  inflating: /content/flags/om.png   \n",
            "  inflating: /content/flags/pa.png   \n",
            "  inflating: /content/flags/pe.png   \n",
            "  inflating: /content/flags/pf.png   \n",
            " extracting: /content/flags/pg.png   \n",
            " extracting: /content/flags/ph.png   \n",
            "  inflating: /content/flags/pk.png   \n",
            "  inflating: /content/flags/pl.png   \n",
            " extracting: /content/flags/pm.png   \n",
            " extracting: /content/flags/pn.png   \n",
            "  inflating: /content/flags/pr.png   \n",
            "  inflating: /content/flags/ps.png   \n",
            "  inflating: /content/flags/pt.png   \n",
            " extracting: /content/flags/pw.png   \n",
            "  inflating: /content/flags/py.png   \n",
            "  inflating: /content/flags/qa.png   \n",
            " extracting: /content/flags/re.png   \n",
            "  inflating: /content/flags/ro.png   \n",
            " extracting: /content/flags/rs.png   \n",
            "  inflating: /content/flags/ru.png   \n",
            "  inflating: /content/flags/rw.png   \n",
            " extracting: /content/flags/sa.png   \n",
            "  inflating: /content/flags/sb.png   \n",
            " extracting: /content/flags/sc.png   \n",
            " extracting: /content/flags/sd.png   \n",
            "  inflating: /content/flags/se.png   \n",
            "  inflating: /content/flags/sg.png   \n",
            "  inflating: /content/flags/sh.png   \n",
            "  inflating: /content/flags/si.png   \n",
            "  inflating: /content/flags/sj.png   \n",
            "  inflating: /content/flags/sk.png   \n",
            "  inflating: /content/flags/sl.png   \n",
            "  inflating: /content/flags/sm.png   \n",
            "  inflating: /content/flags/sn.png   \n",
            "  inflating: /content/flags/so.png   \n",
            "  inflating: /content/flags/sr.png   \n",
            " extracting: /content/flags/ss.png   \n",
            "  inflating: /content/flags/st.png   \n",
            "  inflating: /content/flags/sv.png   \n",
            "  inflating: /content/flags/sx.png   \n",
            "  inflating: /content/flags/sy.png   \n",
            "  inflating: /content/flags/sz.png   \n",
            "  inflating: /content/flags/tc.png   \n",
            "  inflating: /content/flags/td.png   \n",
            "  inflating: /content/flags/tf.png   \n",
            "  inflating: /content/flags/tg.png   \n",
            "  inflating: /content/flags/th.png   \n",
            "  inflating: /content/flags/tj.png   \n",
            " extracting: /content/flags/tk.png   \n",
            " extracting: /content/flags/tl.png   \n",
            " extracting: /content/flags/tm.png   \n",
            " extracting: /content/flags/tn.png   \n",
            "  inflating: /content/flags/to.png   \n",
            " extracting: /content/flags/tr.png   \n",
            " extracting: /content/flags/tt.png   \n",
            " extracting: /content/flags/tv.png   \n",
            "  inflating: /content/flags/tw.png   \n",
            "  inflating: /content/flags/tz.png   \n",
            "  inflating: /content/flags/ua.png   \n",
            "  inflating: /content/flags/ug.png   \n",
            "  inflating: /content/flags/um.png   \n",
            "  inflating: /content/flags/us.png   \n",
            "  inflating: /content/flags/uy.png   \n",
            "  inflating: /content/flags/uz.png   \n",
            "  inflating: /content/flags/va.png   \n",
            "  inflating: /content/flags/vc.png   \n",
            "  inflating: /content/flags/ve.png   \n",
            " extracting: /content/flags/vg.png   \n",
            "  inflating: /content/flags/vi.png   \n",
            " extracting: /content/flags/vn.png   \n",
            " extracting: /content/flags/vu.png   \n",
            "  inflating: /content/flags/wf.png   \n",
            "  inflating: /content/flags/ws.png   \n",
            " extracting: /content/flags/xk.png   \n",
            "  inflating: /content/flags/ye.png   \n",
            "  inflating: /content/flags/yt.png   \n",
            "  inflating: /content/flags/za.png   \n",
            "  inflating: /content/flags/zm.png   \n",
            "  inflating: /content/flags/zw.png   \n",
            "rm: cannot remove '/content/dogs': No such file or directory\n",
            "Archive:  /content/dogs.zip\n",
            "  inflating: /content/dogs/01c6b7230c.jpg  \n",
            "  inflating: /content/dogs/024a037366.jpg  \n",
            "  inflating: /content/dogs/05b151f72d.jpg  \n",
            "  inflating: /content/dogs/084a6888d7.jpg  \n",
            "  inflating: /content/dogs/0a73823599.jpg  \n",
            "  inflating: /content/dogs/0b6670809d.jpg  \n",
            "  inflating: /content/dogs/0be3797d3d.jpg  \n",
            "  inflating: /content/dogs/0d33157df8.jpg  \n",
            "  inflating: /content/dogs/0df912089d.jpg  \n",
            "  inflating: /content/dogs/0ff0ba075e.jpg  \n",
            "  inflating: /content/dogs/17f6784a37.jpg  \n",
            "  inflating: /content/dogs/1c768b0135.jpg  \n",
            "  inflating: /content/dogs/1d97824ca1.jpg  \n",
            "  inflating: /content/dogs/22f99b4396.jpg  \n",
            "  inflating: /content/dogs/23c5c07d8d.jpg  \n",
            "  inflating: /content/dogs/26c7056eda.jpg  \n",
            "  inflating: /content/dogs/2a74dd288c.jpg  \n",
            "  inflating: /content/dogs/2a8a6a6050.jpg  \n",
            "  inflating: /content/dogs/2afd300e67.jpg  \n",
            "  inflating: /content/dogs/2cfd581819.jpg  \n",
            "  inflating: /content/dogs/2f0043ea55.jpg  \n",
            "  inflating: /content/dogs/36c2df0788.jpg  \n",
            "  inflating: /content/dogs/3f956b8d4c.jpg  \n",
            "  inflating: /content/dogs/3fb7d7c4ef.jpg  \n",
            "  inflating: /content/dogs/41f5881bb6.jpg  \n",
            "  inflating: /content/dogs/44a1bbc202.jpg  \n",
            "  inflating: /content/dogs/4aacd195b5.jpg  \n",
            "  inflating: /content/dogs/4db9316411.jpg  \n",
            "  inflating: /content/dogs/4ddebbca9a.jpg  \n",
            "  inflating: /content/dogs/4df813f7f1.jpg  \n",
            "  inflating: /content/dogs/4fdbeb98ad.jpg  \n",
            "  inflating: /content/dogs/58b4804b9e.jpg  \n",
            "  inflating: /content/dogs/5c641a8bde.jpg  \n",
            "  inflating: /content/dogs/5d961159a7.jpg  \n",
            "  inflating: /content/dogs/62ebcbaf99.jpg  \n",
            "  inflating: /content/dogs/63b74efb32.jpg  \n",
            "  inflating: /content/dogs/6a5c5cce2c.jpg  \n",
            "  inflating: /content/dogs/6afa462776.jpg  \n",
            "  inflating: /content/dogs/6be2081479.jpg  \n",
            "  inflating: /content/dogs/76bad2abea.jpg  \n",
            "  inflating: /content/dogs/7a42ed2c8d.jpg  \n",
            "  inflating: /content/dogs/7a47b70b45.jpg  \n",
            "  inflating: /content/dogs/7b0c4e73fd.jpg  \n",
            "  inflating: /content/dogs/7bee213463.jpg  \n",
            "  inflating: /content/dogs/7c43d5ca9e.jpg  \n",
            "  inflating: /content/dogs/7ca88b95e5.jpg  \n",
            "  inflating: /content/dogs/7f45bd0030.jpg  \n",
            "  inflating: /content/dogs/82a669ccf6.jpg  \n",
            "  inflating: /content/dogs/86ca756626.jpg  \n",
            "  inflating: /content/dogs/87bfbc889f.jpg  \n",
            "  inflating: /content/dogs/88dcdf1906.jpg  \n",
            "  inflating: /content/dogs/89af0e8454.jpg  \n",
            "  inflating: /content/dogs/8c6a936c9a.jpg  \n",
            "  inflating: /content/dogs/8deb324d92.jpg  \n",
            "  inflating: /content/dogs/8edafffd38.jpg  \n",
            "  inflating: /content/dogs/9a8262798f.jpg  \n",
            "  inflating: /content/dogs/9ac386dfb6.jpg  \n",
            "  inflating: /content/dogs/9aeb8c737b.jpg  \n",
            "  inflating: /content/dogs/9bd48a19a4.jpg  \n",
            "  inflating: /content/dogs/9efd18dd6c.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My model"
      ],
      "metadata": {
        "id": "o6Z0akAFSqPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model that is the same as clip, having removed its last layers.\n",
        "# Its output is the input of the last convolution layer, whose activations we want to test\n",
        "# The code for the original CLIP model can be found in https://github.com/openai/CLIP/blob/main/clip/model.py\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, final = False):\n",
        "        super().__init__()\n",
        "\n",
        "        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n",
        "\n",
        "        if not final:\n",
        "          self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n",
        "          self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "          self.relu3 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.downsample = None\n",
        "        self.stride = stride\n",
        "        self.final = final\n",
        "\n",
        "        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n",
        "            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n",
        "            self.downsample = nn.Sequential(OrderedDict([\n",
        "                (\"-1\", nn.AvgPool2d(stride)),\n",
        "                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n",
        "                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        identity = x\n",
        "\n",
        "        out = self.relu1(self.bn1(self.conv1(x)))\n",
        "        out = self.relu2(self.bn2(self.conv2(out)))\n",
        "        out = self.avgpool(out)\n",
        "\n",
        "        if not self.final:\n",
        "          out = self.bn3(self.conv3(out))\n",
        "\n",
        "          if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "          out += identity\n",
        "          out = self.relu3(out)\n",
        "        return out\n",
        "\n",
        "class MyModifiedResNet(nn.Module):    # I have removed some final layers, up to before the last convolution\n",
        "    \"\"\"\n",
        "    A ResNet class that is similar to torchvision's but contains the following changes:\n",
        "    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n",
        "    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n",
        "    - The final pooling layer is a QKV attention instead of an average pool\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layers, input_resolution=224, width=64):\n",
        "        super().__init__()\n",
        "        self.input_resolution = input_resolution\n",
        "\n",
        "        # the 3-layer stem\n",
        "        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(width // 2)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(width // 2)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(width)\n",
        "        self.relu3 = nn.ReLU(inplace=True)\n",
        "        self.avgpool = nn.AvgPool2d(2)\n",
        "\n",
        "        # residual layers\n",
        "        self._inplanes = width  # this is a *mutable* variable used during construction\n",
        "        self.layer1 = self._make_layer(width, layers[0])\n",
        "        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(width * 8, layers[3], stride=2, final = True)\n",
        "\n",
        "    def _make_layer(self, planes, blocks, stride=1, final = False):\n",
        "        layers = [Bottleneck(self._inplanes, planes, stride)]\n",
        "\n",
        "        self._inplanes = planes * Bottleneck.expansion\n",
        "\n",
        "        if final:\n",
        "          blocks -= 1\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(Bottleneck(self._inplanes, planes))\n",
        "\n",
        "        if final:\n",
        "          layers.append(Bottleneck(self._inplanes, planes, final = True))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        def stem(x):\n",
        "            x = self.relu1(self.bn1(self.conv1(x)))\n",
        "            x = self.relu2(self.bn2(self.conv2(x)))\n",
        "            x = self.relu3(self.bn3(self.conv3(x)))\n",
        "            x = self.avgpool(x)\n",
        "            return x\n",
        "\n",
        "        x = x.type(self.conv1.weight.dtype)\n",
        "        x = stem(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "6eiRjtmz0IME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mymodel = MyModifiedResNet([4,6,10,6], width = 80)   # these are the parameters used in CLIP RN50x4\n",
        "mymodel.load_state_dict(model.visual.state_dict(), strict = False, assign = True)    # copy model parameters\n",
        "mymodel.cuda()\n",
        "mymodel.trainable = False       # freeze mymodel parameters, and enter evaluation mode\n",
        "mymodel.eval()\n",
        "m = mymodel.requires_grad_(False)"
      ],
      "metadata": {
        "id": "5mTyLiTk83Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convlayer = model.visual.layer4[5].conv3.half()\n",
        "param_tensor = list(convlayer.parameters())[0]   # get last conv layer parameters\n",
        "print(param_tensor.shape)\n",
        "param_tensor = param_tensor.mean(dim = (2,3))    # set their dimension to 2560x640, we have 2560 filters of dimension 640 (x1x1)\n",
        "print(param_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-4L8z1I8-oB",
        "outputId": "13f8f833-d9ce-47da-ab60-ddee0eec4ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2560, 640, 1, 1])\n",
            "torch.Size([2560, 640])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Maximizing text"
      ],
      "metadata": {
        "id": "9tkvplcy8pgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the tokenizer\n",
        "\n",
        "my_tokenizer = clip.clip._tokenizer\n",
        "\n",
        "txt_input = clip.tokenize([\"A photo of clouds\"]).cuda()\n",
        "my_list = [i.item() for i in txt_input[0]]\n",
        "res = my_tokenizer.decode(my_list)\n",
        "start = \"<|startoftext|>\"\n",
        "end = \" <|endoftext|>\"\n",
        "result = res[res.find(start)+len(start):res.rfind(end)]      # get the text between the first <startoftext> and the last <endoftext>\n",
        "\n",
        "print(txt_input)\n",
        "print(txt_input.argmax(dim=-1))\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "Zw_0Rj4TSyAJ",
        "outputId": "f264c62c-e413-4feb-90ab-f9acdb3a6eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[49406,   320,  1125,   539,  6244, 49407,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0]], device='cuda:0',\n",
            "       dtype=torch.int32)\n",
            "tensor([5], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a photo of clouds'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_tokenizer = clip.clip._tokenizer\n",
        "\n",
        "photo = [preprocess(Image.open(\"/content/trump1/im-799917.jfif\"))]  # select an image and encode it, this will serve as the optimization target\n",
        "im_input = torch.stack(photo).cuda()\n",
        "#im_input = input.clone().detach()\n",
        "target = model.encode_image(im_input)\n",
        "target /= target.norm(dim=-1, keepdim=True)"
      ],
      "metadata": {
        "id": "q1ZmygRhXaen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on input\n",
        "\n",
        "random_seed = 20\n",
        "num_epochs = 20   # how many optim epochs to take\n",
        "iter = 100   # how many iterations at each epoch\n",
        "learning_rate = 2\n",
        "\n",
        "\n",
        "def get_input_emb(text = \"Donald Trump\"):    # Get token_input and input embedding from text (the input will be used for training)\n",
        "  with torch.no_grad():\n",
        "    token_input = clip.tokenize([text]).cuda()\n",
        "    input2 = model.token_embedding(token_input).type(model.dtype)\n",
        "    input2.requires_grad_(True)  # input will be used for training\n",
        "  return token_input, input2\n",
        "\n",
        "token_input, input2 = get_input_emb(\"This is a sentence\")\n",
        "start_input = input2.clone().detach()\n",
        "\n",
        "def my_encode_text(text_embedding):  # my_encode_text is the same as the model's encode_text, but it gets as input the text_embedding instead of the text tokens\n",
        "  x = text_embedding + model.positional_embedding.type(model.dtype)\n",
        "  x = x.permute(1, 0, 2)  # NLD -> LND\n",
        "  x = model.transformer(x)\n",
        "  x = x.permute(1, 0, 2)  # LND -> NLD\n",
        "  x = model.ln_final(x).type(model.dtype)\n",
        "  x = x[torch.arange(x.shape[0]), token_input.argmax(dim=-1)] @ model.text_projection\n",
        "  x = x / x.norm(dim=-1, keepdim=True)\n",
        "  return x\n",
        "\n",
        "def get_nearest(input):     # Get the nearest token to input embedding\n",
        "  nearest = []\n",
        "  for xi in input[0]:\n",
        "    distance = torch.norm(model.token_embedding.weight.data - xi, dim=1)   # this tensor holds the distance of each token embedding to the embedding xi of our input\n",
        "    nearest.append(torch.argmin(distance))                                 # the closest token indices are stored in nearest list\n",
        "  nearest = torch.Tensor(nearest).type(torch.int32)\n",
        "\n",
        "  res = my_tokenizer.decode(nearest.tolist())           # decode the tokens to corresponding text to display it\n",
        "  start = \"<|startoftext|>\"\n",
        "  end = \"<|endoftext|>\"\n",
        "  result = res[res.find(start)+len(start):res.rfind(end)]\n",
        "  return result\n",
        "\n",
        "losses = []     # store the losses at the end of each epoch and the text results\n",
        "results = []\n",
        "my_tqdm = tqdm(range(num_epochs))\n",
        "for step in my_tqdm:\n",
        "  torch.manual_seed(step * random_seed)           # get a different seed at each iteration to prevent repeating the same steps\n",
        "  my_lr = learning_rate #* (1 - step / num_steps / 2)\n",
        "  optim = torch.optim.SGD([input2], lr=my_lr)\n",
        "  for _ in range(iter):\n",
        "    output = my_encode_text(input2)     # get model output\n",
        "    loss = - output @ target.T          # loss is minus inner product of output and target, as we want them to be as similar as possible (they have already been normalized)\n",
        "    loss.backward(retain_graph=True)    # retain graph to solve some errors during runtime\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    with torch.no_grad():\n",
        "      input2[0][0] = start_input[0][0]     # freeze all input weights expect for tokens 1-4, in this way, we end up only training tokens 1-4\n",
        "      input2[0][5:] = start_input[0][5:]\n",
        "  result = get_nearest(input2)\n",
        "  token_input, input2 = get_input_emb(result)\n",
        "  loss = (- my_encode_text(input2) @ target.T).item()\n",
        "  losses.append(loss)\n",
        "  results.append(result)\n",
        "  my_tqdm.set_postfix(loss = loss, result = result)"
      ],
      "metadata": {
        "id": "OaNT9Wy8WurV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6738635b-a7d3-4387-fb04-6c17abe740e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:42<00:00,  2.14s/it, loss=-0.286, result=<|startoftext|>appetdrainthe(#]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = [x + str(y) for y, x in sorted(list(set(zip(losses, results))))]  # print the text result and the loss at the end of each epoch, sorted by increasing loss\n",
        "sorted_results = [x for _, x in sorted(list(set(zip(losses, results))))]\n",
        "a"
      ],
      "metadata": {
        "id": "CDYpnGiEO04s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55570f92-5d1e-47d7-e49d-df77af3879be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['realdonaldtrump <|startoftext|><|startoftext|>parkrun -0.343017578125',\n",
              " 'nafta <|startoftext|><|startoftext|>malaria -0.340576171875',\n",
              " 'barron <|startoftext|>clusive <|startoftext|>-0.322021484375',\n",
              " 'gopdebate solar pper <|startoftext|>-0.3134765625',\n",
              " 'comey mariano wolfpack <|startoftext|>-0.30712890625',\n",
              " 'ðŸ‡¸ðŸ‡ª reveals ludwig <|startoftext|>-0.30224609375',\n",
              " '<|startoftext|><|startoftext|>recruiting <|startoftext|>-0.298095703125',\n",
              " 'desk <|startoftext|>striking supposed -0.296875',\n",
              " 'hungover clown harvey <|startoftext|>-0.296875',\n",
              " 'defeat lulu <|startoftext|><|startoftext|>-0.28955078125',\n",
              " '<|startoftext|><|startoftext|><|startoftext|>canelo -0.2880859375',\n",
              " '<|startoftext|>appetdrainthe(# -0.2861328125',\n",
              " 'melania ðŸ‡«ðŸ‡· <|startoftext|>sculpting -0.284912109375',\n",
              " 'ðŸ”´poche dex <|startoftext|>-0.281005859375',\n",
              " \"hungover sympathi:'penny -0.280029296875\",\n",
              " 'rameshvettel benghazi cobra -0.278076171875',\n",
              " 'saffron ðŸ“·@ myo<|startoftext|>-0.270751953125',\n",
              " 'hungover arias barclay ï¿½-0.264892578125',\n",
              " 'manage woodbridge ssnhq <|startoftext|>-0.26318359375',\n",
              " 'woodson icanskymap <|startoftext|>-0.248779296875']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here, we compare our result to the result of the result from an optimal text that describes the image\n",
        "\n",
        "t0 = model.encode_text(clip.tokenize([sorted_results[0]]).cuda())\n",
        "t0 /= t0.norm(dim=-1, keepdim=True)\n",
        "my_result = torch.norm(t0 @ target.T).item()\n",
        "\n",
        "t1 = model.encode_text(clip.tokenize([\"Donald Trump\"]).cuda())\n",
        "t1 /= t1.norm(dim=-1, keepdim=True)\n",
        "opt_result = torch.norm(t1 @ target.T).item()\n",
        "\n",
        "print(my_result, opt_result)"
      ],
      "metadata": {
        "id": "nO9JjVaaQuRn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec4aee71-a641-49fd-8c8e-085de50268c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.343017578125 0.3359375\n"
          ]
        }
      ]
    }
  ]
}